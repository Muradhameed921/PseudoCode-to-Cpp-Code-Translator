{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "**Attempt 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-27T06:41:45.081461Z",
     "iopub.status.busy": "2025-02-27T06:41:45.081149Z",
     "iopub.status.idle": "2025-02-27T06:41:50.752451Z",
     "shell.execute_reply": "2025-02-27T06:41:50.751662Z",
     "shell.execute_reply.started": "2025-02-27T06:41:45.081435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.42.2-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (19.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.23->streamlit) (2.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.19.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.23->streamlit) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.23->streamlit) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.23->streamlit) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.23->streamlit) (2024.2.0)\n",
      "Downloading streamlit-1.42.2-py2.py3-none-any.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
      "Successfully installed pydeck-0.9.1 streamlit-1.42.2\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-27T06:41:50.754078Z",
     "iopub.status.busy": "2025-02-27T06:41:50.753765Z",
     "iopub.status.idle": "2025-02-27T06:41:56.597993Z",
     "shell.execute_reply": "2025-02-27T06:41:56.597316Z",
     "shell.execute_reply.started": "2025-02-27T06:41:50.754047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "import streamlit as st\n",
    "from transformers import BertTokenizer  # Import BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-27T06:44:03.075140Z",
     "iopub.status.busy": "2025-02-27T06:44:03.074830Z",
     "iopub.status.idle": "2025-02-27T06:46:03.343558Z",
     "shell.execute_reply": "2025-02-27T06:46:03.342705Z",
     "shell.execute_reply.started": "2025-02-27T06:44:03.075118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Vocabulary Size (BERT): 30522\n",
      "Code Vocabulary Size (BERT): 30522\n",
      "Number of training examples: 216225\n",
      "Example text batch shape: torch.Size([128, 128])\n",
      "Example code batch shape: torch.Size([128, 128])\n",
      "Using device: cuda\n",
      "Model initialized with 27,425,082 parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "def load_spoc_data(file_path, text_col='text', code_col='code'):\n",
    "    \"\"\"Loads SPoC data from a TSV file and returns text and code pairs.\"\"\"\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    # Filter out rows where either 'text' or 'code' is NaN\n",
    "    df_clean = df.dropna(subset=[text_col, code_col])\n",
    "    return df_clean[text_col].tolist(), df_clean[code_col].tolist()\n",
    "\n",
    "# Load BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Special tokens from BERT tokenizer (no need to define our own UNK, PAD, BOS, EOS explicitly now)\n",
    "PAD_TOKEN = bert_tokenizer.pad_token\n",
    "UNK_TOKEN = bert_tokenizer.unk_token\n",
    "BOS_TOKEN = bert_tokenizer.cls_token  # Using BERT's CLS token as BOS - you can adjust if needed\n",
    "EOS_TOKEN = bert_tokenizer.sep_token  # Using BERT's SEP token as EOS - you can adjust if needed\n",
    "\n",
    "\n",
    "def tokenize_and_numericalize_bert(texts, codes, tokenizer, max_len=128):\n",
    "    \"\"\"Tokenizes, numericalizes, and pads text and code sequences using BERT tokenizer.\"\"\"\n",
    "    text_sequences_numericalized = []\n",
    "    code_sequences_numericalized = []\n",
    "\n",
    "    for text, code in zip(texts, codes):\n",
    "        # Tokenize and numericalize text using BERT tokenizer\n",
    "        encoded_text = tokenizer.encode(\n",
    "            BOS_TOKEN + \" \" + text + \" \" + EOS_TOKEN, # Add BOS and EOS tokens\n",
    "            add_special_tokens=False, # already added manually\n",
    "            max_length=max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length' # Pad to max_length\n",
    "        )\n",
    "        encoded_code = tokenizer.encode(\n",
    "            BOS_TOKEN + \" \" + code + \" \" + EOS_TOKEN, # Add BOS and EOS tokens\n",
    "            add_special_tokens=False, # already added manually\n",
    "            max_length=max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length' # Pad to max_length\n",
    "        )\n",
    "\n",
    "        text_sequences_numericalized.append(torch.tensor(encoded_text))\n",
    "        code_sequences_numericalized.append(torch.tensor(encoded_code))\n",
    "\n",
    "\n",
    "    text_sequences_padded = torch.stack(text_sequences_numericalized) # Stacking to create tensor batch\n",
    "    code_sequences_padded = torch.stack(code_sequences_numericalized) # Stacking to create tensor batch\n",
    "\n",
    "\n",
    "    return text_sequences_padded, code_sequences_padded\n",
    "\n",
    "class SPOCDataset(Dataset):\n",
    "    def __init__(self, text_sequences, code_sequences):\n",
    "        self.text_sequences = text_sequences\n",
    "        self.code_sequences = code_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_sequences[idx], self.code_sequences[idx]\n",
    "\n",
    "\n",
    "# --- Main Data Preparation ---\n",
    "train_file_path = '/kaggle/input/spoc-train-tsv/spoc-train.tsv' # Assuming dataset is in 'train' directory\n",
    "train_texts, train_codes = load_spoc_data(train_file_path)\n",
    "\n",
    "\n",
    "# Numericalize and pad the actual training data using BERT tokenizer\n",
    "MAX_SEQ_LEN = 128 # Define a maximum sequence length\n",
    "train_text_numericalized, train_code_numericalized = tokenize_and_numericalize_bert(\n",
    "    train_texts, train_codes, bert_tokenizer, max_len=MAX_SEQ_LEN\n",
    ")\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = SPOCDataset(train_text_numericalized, train_code_numericalized)\n",
    "BATCH_SIZE = 128 # Define batch size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# --- Vocabulary Sizes ---\n",
    "TEXT_VOCAB_SIZE = bert_tokenizer.vocab_size\n",
    "CODE_VOCAB_SIZE = bert_tokenizer.vocab_size # BERT tokenizer is shared - you might use separate if needed, or different BERT models\n",
    "# In this case, we are using the same tokenizer for both source and target vocab, which is a simplification and might work for pseudocode to code.\n",
    "\n",
    "print(f\"Text Vocabulary Size (BERT): {TEXT_VOCAB_SIZE}\")\n",
    "print(f\"Code Vocabulary Size (BERT): {CODE_VOCAB_SIZE}\")\n",
    "print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "\n",
    "# Example of data from DataLoader\n",
    "text_batch, code_batch = next(iter(train_dataloader))\n",
    "print(\"Example text batch shape:\", text_batch.shape) # [Batch_size, Seq_len]\n",
    "print(\"Example code batch shape:\", code_batch.shape) # [Batch_size, Seq_len]\n",
    "\n",
    "\n",
    "# --- Placeholders for Tokenizers and Vocabs for later use ---\n",
    "ar_tokenizer = bert_tokenizer # Using BERT tokenizer for pseudocode\n",
    "en_tokenizer = bert_tokenizer # Using BERT tokenizer for C++ code (you could use a different one if desired)\n",
    "ar_vocab_size = TEXT_VOCAB_SIZE\n",
    "en_vocab_size = CODE_VOCAB_SIZE\n",
    "\n",
    "\n",
    "# --- Device Configuration ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# --- Transformer Model Implementation (Reference Code - Corrected _init_ to __init__) ---\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, d_model=256, num_heads=8,\n",
    "                 num_layers=3, d_ff=512, dropout=0.1, max_len=128):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Positional encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "        # Encoder and decoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(d_model, trg_vocab_size)\n",
    "\n",
    "        # Other parameters\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        self.src_pad_idx = bert_tokenizer.pad_token_id # Use BERT pad token id\n",
    "        self.trg_pad_idx = bert_tokenizer.pad_token_id # Use BERT pad token id\n",
    "\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # Create mask for padding in source (1 for tokens, 0 for padding)\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        # Create mask for padding and look-ahead\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        # Padding mask (1 for tokens, 0 for padding)\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Look-ahead mask (lower triangular matrix)\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=trg.device)).bool()\n",
    "        trg_sub_mask = trg_sub_mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Combine masks\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # Create masks\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # Source embedding and positional encoding\n",
    "        src = self.src_embedding(src) * self.scale\n",
    "        src = src + self.pe[:, :src.size(1)].to(src.device)\n",
    "        src = self.dropout(src)\n",
    "\n",
    "        # Encoder\n",
    "        enc_output = src\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        # Target embedding and positional encoding\n",
    "        trg = self.trg_embedding(trg) * self.scale\n",
    "        trg = trg + self.pe[:, :trg.size(1)].to(trg.device)\n",
    "        trg = self.dropout(trg)\n",
    "\n",
    "        # Decoder\n",
    "        dec_output = trg\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, trg_mask, src_mask)\n",
    "\n",
    "        # Output\n",
    "        output = self.output_layer(dec_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Multi-Head Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]))\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        # Linear projections and reshape\n",
    "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Attention\n",
    "        energy = torch.matmul(q, k.permute(0, 1, 3, 2)) / self.scale.to(q.device)\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = self.dropout(F.softmax(energy, dim=-1))\n",
    "        output = torch.matmul(attention, v)\n",
    "\n",
    "        # Reshape and concat heads\n",
    "        output = output.permute(0, 2, 1, 3).contiguous()\n",
    "        output = output.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Position-wise Feed Forward Network\n",
    "class PositionwiseFeedforward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedforward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
    "\n",
    "# Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedforward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_out = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "\n",
    "        # Feed forward with residual connection and layer norm\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_out))\n",
    "\n",
    "        return x\n",
    "\n",
    "# Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.enc_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedforward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, trg_mask, src_mask):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        self_attn_out = self.self_attn(x, x, x, trg_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_out))\n",
    "\n",
    "        # Encoder-decoder attention with residual connection and layer norm\n",
    "        enc_attn_out = self.enc_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(enc_attn_out))\n",
    "\n",
    "        # Feed forward with residual connection and layer norm\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_out))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Model Initialization ---\n",
    "model = Transformer(\n",
    "    src_vocab_size=ar_vocab_size,\n",
    "    trg_vocab_size=en_vocab_size,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    num_layers=3,\n",
    "    d_ff=512,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f'Model initialized with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} parameters')\n",
    "# --- Training Setup ---\n",
    "LEARNING_RATE = 0.0001  # You can tune this\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ar_tokenizer.pad_token_id) # or en_tokenizer.pad_token_id - they are the same\n",
    "\n",
    "NUM_EPOCHS = 20 # <--- SPECIFY NUMBER OF EPOCHS HERE\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train() # Set model to training mode\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (src, trg) in enumerate(dataloader):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "\n",
    "        output = model(src, trg[:, :-1]) # Don't feed EOS token of target to decoder input, output shape [batch_size, trg_len-1, trg_vocab_size]\n",
    "\n",
    "        # Reshape for loss calculation\n",
    "        output_reshape = output.contiguous().view(-1, output.shape[-1]) # [batch_size * (trg_len-1), trg_vocab_size]\n",
    "        trg_reshape = trg[:, 1:].contiguous().view(-1) # # [batch_size * (trg_len-1)] - shifted target, don't include BOS token in target for loss\n",
    "\n",
    "        loss = criterion(output_reshape, trg_reshape)\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Update weights\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader) # Average loss per batch\n",
    "\n",
    "def train(model, train_dataloader, optimizer, criterion, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "        end_time = time.time()\n",
    "        epoch_mins = int((end_time - start_time) / 60)\n",
    "        epoch_secs = int((end_time - start_time) - (epoch_mins * 60))\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-27T06:46:03.345100Z",
     "iopub.status.busy": "2025-02-27T06:46:03.344856Z",
     "iopub.status.idle": "2025-02-27T09:38:56.265687Z",
     "shell.execute_reply": "2025-02-27T09:38:56.264879Z",
     "shell.execute_reply.started": "2025-02-27T06:46:03.345080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 8m 37s\n",
      "\tTrain Loss: 1.610 | Train PPL:   5.002\n",
      "Epoch: 02 | Time: 8m 38s\n",
      "\tTrain Loss: 0.670 | Train PPL:   1.955\n",
      "Epoch: 03 | Time: 8m 39s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.695\n",
      "Epoch: 04 | Time: 8m 39s\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.566\n",
      "Epoch: 05 | Time: 8m 39s\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
      "Epoch: 06 | Time: 8m 39s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "Epoch: 07 | Time: 8m 38s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "Epoch: 08 | Time: 8m 38s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "Epoch: 09 | Time: 8m 38s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.336\n",
      "Epoch: 10 | Time: 8m 38s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "Epoch: 11 | Time: 8m 39s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "Epoch: 12 | Time: 8m 38s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "Epoch: 13 | Time: 8m 37s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "Epoch: 14 | Time: 8m 37s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.262\n",
      "Epoch: 15 | Time: 8m 37s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "Epoch: 16 | Time: 8m 38s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "Epoch: 17 | Time: 8m 38s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "Epoch: 18 | Time: 8m 38s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "Epoch: 19 | Time: 8m 39s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "Epoch: 20 | Time: 8m 38s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n"
     ]
    }
   ],
   "source": [
    "# --- Run Training ---\n",
    "train(model, train_dataloader, optimizer, criterion, NUM_EPOCHS, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-27T09:46:10.206911Z",
     "iopub.status.busy": "2025-02-27T09:46:10.206481Z",
     "iopub.status.idle": "2025-02-27T09:46:10.366781Z",
     "shell.execute_reply": "2025-02-27T09:46:10.365894Z",
     "shell.execute_reply.started": "2025-02-27T09:46:10.206879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at spoc_transformer_1.pth\n"
     ]
    }
   ],
   "source": [
    "# Function to save the trained model\n",
    "def save_model(model, path=\"spoc_transformer_1.pth\"):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved at {path}\")\n",
    "\n",
    "# Call this function after training\n",
    "save_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-27T09:46:16.046450Z",
     "iopub.status.busy": "2025-02-27T09:46:16.046130Z",
     "iopub.status.idle": "2025-02-27T09:46:16.344791Z",
     "shell.execute_reply": "2025-02-27T09:46:16.343810Z",
     "shell.execute_reply.started": "2025-02-27T09:46:16.046424Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from spoc_transformer_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-b18954b47e83>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "# Function to load the trained model\n",
    "def load_model(model, path=\"spoc_transformer_1.pth\"):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model\n",
    "\n",
    "# Load the trained model\n",
    "model = Transformer(src_vocab_size=TEXT_VOCAB_SIZE, trg_vocab_size=CODE_VOCAB_SIZE)\n",
    "model.to(device)\n",
    "model = load_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-27T09:46:25.807225Z",
     "iopub.status.busy": "2025-02-27T09:46:25.806946Z",
     "iopub.status.idle": "2025-02-27T09:46:25.812931Z",
     "shell.execute_reply": "2025-02-27T09:46:25.812137Z",
     "shell.execute_reply.started": "2025-02-27T09:46:25.807205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_code(model, tokenizer, input_text, max_len=128):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize input\n",
    "        input_ids = tokenizer.encode(BOS_TOKEN + \" \" + input_text + \" \" + EOS_TOKEN, \n",
    "                                     add_special_tokens=False, \n",
    "                                     max_length=max_len, \n",
    "                                     truncation=True, \n",
    "                                     padding='max_length')\n",
    "        input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        \n",
    "        # Start with <BOS> token\n",
    "        output_tokens = [tokenizer.cls_token_id]  # Using BERT CLS as BOS\n",
    "        for _ in range(max_len):\n",
    "            output_tensor = torch.tensor(output_tokens).unsqueeze(0).to(device)\n",
    "            predictions = model(input_tensor, output_tensor)\n",
    "            next_token = predictions.argmax(dim=-1)[:, -1].item()  # Get most probable token\n",
    "            \n",
    "            if next_token == tokenizer.sep_token_id:  # Stop if <EOS> is generated\n",
    "                break\n",
    "\n",
    "            output_tokens.append(next_token)\n",
    "\n",
    "        # Convert token IDs back to text\n",
    "        generated_code = tokenizer.decode(output_tokens[1:], skip_special_tokens=True)  # Skip BOS token\n",
    "        return generated_code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-27T10:16:42.060844Z",
     "iopub.status.busy": "2025-02-27T10:16:42.060491Z",
     "iopub.status.idle": "2025-02-27T10:16:42.249516Z",
     "shell.execute_reply": "2025-02-27T10:16:42.248854Z",
     "shell.execute_reply.started": "2025-02-27T10:16:42.060820Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated C++ Code:\n",
      " for ( int i = 1 ; i < = n ; i + + ) cout < < i * i < < endl ;\n"
     ]
    }
   ],
   "source": [
    "# Example Test\n",
    "test_pseudocode = \"for i from 1 to n: print i*i\"\n",
    "generated_cpp_code = generate_code(model, bert_tokenizer, test_pseudocode)\n",
    "print(\"Generated C++ Code:\\n\", generated_cpp_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-27T10:15:12.877695Z",
     "iopub.status.busy": "2025-02-27T10:15:12.877335Z",
     "iopub.status.idle": "2025-02-27T10:15:13.082397Z",
     "shell.execute_reply": "2025-02-27T10:15:13.081695Z",
     "shell.execute_reply.started": "2025-02-27T10:15:12.877667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated C++ Code:\n",
      " for ( int i = 1 ; i < = 10 ; i + + ) { cout < < name [ i ] < < endl ; }\n"
     ]
    }
   ],
   "source": [
    "# Example Test\n",
    "test_pseudocode = \"for i from 1 to 10 print name array i index\"\n",
    "generated_cpp_code = generate_code(model, bert_tokenizer, test_pseudocode)\n",
    "print(\"Generated C++ Code:\\n\", generated_cpp_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-02-27T10:18:28.541468Z",
     "iopub.status.busy": "2025-02-27T10:18:28.541172Z",
     "iopub.status.idle": "2025-02-27T10:18:28.610342Z",
     "shell.execute_reply": "2025-02-27T10:18:28.609628Z",
     "shell.execute_reply.started": "2025-02-27T10:18:28.541445Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated C++ Code:\n",
      " void max ( int a, b ) {\n"
     ]
    }
   ],
   "source": [
    "# Example Test\n",
    "test_pseudocode = \"function max(a, b): if a > b then return a else return b\"\n",
    "generated_cpp_code = generate_code(model, bert_tokenizer, test_pseudocode)\n",
    "print(\"Generated C++ Code:\\n\", generated_cpp_code)x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6738405,
     "sourceId": 10849773,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
